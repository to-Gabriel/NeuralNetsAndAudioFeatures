{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f6e8748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 20:49:18.289707: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a3fad",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca6a6ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (3014, 200, 200, 3)\n",
      "Validation set: (1710, 200, 200, 3)\n",
      "Test set: (13306, 200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "spectrogram = '/home/madan005/dev/deeplearning/MFCC'\n",
    "\n",
    "\n",
    "train_spectrogram      = os.path.join(spectrogram, 'train')\n",
    "validation_spectrogram = os.path.join(spectrogram, 'dev')\n",
    "test_spectrogram       = os.path.join(spectrogram, 'eval')\n",
    "\n",
    "\n",
    "def load_images_from_folder(folder, label, target_size=(200, 200)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        image = Image.open(filepath)\n",
    "        image = image.convert('RGB')\n",
    "        # resizing\n",
    "        image = image.resize(target_size, Image.LANCZOS)\n",
    "        images.append(np.array(image))\n",
    "        labels.append(label)\n",
    "    return images, labels\n",
    "\n",
    "# data lists\n",
    "X_train, y_train = [], []\n",
    "X_val,   y_val   = [], []\n",
    "X_test,  y_test  = [], []\n",
    "\n",
    "# training data\n",
    "for class_name in ['genuine', 'spoof']:\n",
    "    class_folder = os.path.join(train_spectrogram, class_name)\n",
    "    label = 1 if class_name == 'genuine' else 0\n",
    "    imgs, labels = load_images_from_folder(class_folder, label)\n",
    "    X_train.extend(imgs)\n",
    "    y_train.extend(labels)  \n",
    "# validation data\n",
    "for class_name in ['genuine', 'spoof']:\n",
    "    class_folder = os.path.join(validation_spectrogram, class_name)\n",
    "    label = 1 if class_name == 'genuine' else 0\n",
    "    imgs, labels = load_images_from_folder(class_folder, label)\n",
    "    X_val.extend(imgs)\n",
    "    y_val.extend(labels)\n",
    "# test data\n",
    "for class_name in ['genuine', 'spoof']:\n",
    "    class_folder = os.path.join(test_spectrogram, class_name)\n",
    "    label = 1 if class_name == 'genuine' else 0\n",
    "    imgs, labels = load_images_from_folder(class_folder, label)\n",
    "    X_test.extend(imgs)\n",
    "    y_test.extend(labels)\n",
    "\n",
    "\n",
    "\n",
    "# Normalizing the Images (Min-Max Scaling)\n",
    "X_train = np.array(X_train, dtype='float32') / 255.0\n",
    "X_val   = np.array(X_val,   dtype='float32') / 255.0\n",
    "X_test  = np.array(X_test,  dtype='float32') / 255.0\n",
    "\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val   = np.array(y_val)\n",
    "y_test  = np.array(y_test)\n",
    "\n",
    "# Shapes\n",
    "print(\"Training set:\", X_train.shape)\n",
    "print(\"Validation set:\", X_val.shape)\n",
    "print(\"Test set:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9378723b",
   "metadata": {},
   "source": [
    "class distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf650ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:\n",
      "  spoof: 1507\n",
      "  genuine: 1507\n",
      "\n",
      "Validation:\n",
      "  spoof: 950\n",
      "  genuine: 760\n",
      "\n",
      "Test:\n",
      "  spoof: 12008\n",
      "  genuine: 1298\n"
     ]
    }
   ],
   "source": [
    "# class counts \n",
    "for split_name, labels in [\n",
    "    ('Training', y_train),\n",
    "    ('Validation', y_val),\n",
    "    ('Test', y_test)\n",
    "]:\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"\\n{split_name}:\")\n",
    "    for cls, cnt in zip(unique, counts):\n",
    "        name = 'genuine' if cls == 1 else 'spoof'\n",
    "        print(f\"  {name}: {cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb4fdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4083379/4144618430.py:17: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(*IMG_SIZE, 3), pooling='avg')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "IMG_SIZE    = (200, 200)\n",
    "BATCH_SIZE  = 32\n",
    "LR          = 1e-4\n",
    "L2_REG      = 1e-4\n",
    "\n",
    "\n",
    "# augmentation\n",
    "train_aug = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "# MobileNetV2 backbone\n",
    "base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(*IMG_SIZE, 3), pooling='avg')\n",
    "\n",
    "base_model.trainable = False   # freeze all layers\n",
    "\n",
    "#classification head\n",
    "inputs = layers.Input(shape=(*IMG_SIZE, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "729dc6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">81,984</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m81,984\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,340,033</span> (8.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,340,033\u001b[0m (8.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">82,049</span> (320.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m82,049\u001b[0m (320.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = models.Model(inputs, outputs)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b56ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madan005/miniconda3/envs/my_env/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 297ms/step - accuracy: 0.5402 - loss: 0.7024 - val_accuracy: 0.5006 - val_loss: 0.7007\n",
      "Epoch 2/50\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 376ms/step - accuracy: 0.6095 - loss: 0.6563 - val_accuracy: 0.4906 - val_loss: 0.7240\n",
      "Epoch 3/50\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 344ms/step - accuracy: 0.6267 - loss: 0.6499 - val_accuracy: 0.4953 - val_loss: 0.7434\n",
      "Epoch 4/50\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 261ms/step - accuracy: 0.6416 - loss: 0.6373 - val_accuracy: 0.4953 - val_loss: 0.7332\n",
      "Epoch 5/50\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 209ms/step - accuracy: 0.6503 - loss: 0.6300 - val_accuracy: 0.4912 - val_loss: 0.7593\n",
      "Epoch 6/50\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 211ms/step - accuracy: 0.6501 - loss: 0.6181 - val_accuracy: 0.4842 - val_loss: 0.7866\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=5, restore_best_weights=True )]\n",
    "\n",
    "#Train\n",
    "history = model.fit(\n",
    "    train_aug.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16272b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">81,984</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m81,984\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,340,033</span> (8.93 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,340,033\u001b[0m (8.93 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,134,529</span> (4.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,134,529\u001b[0m (4.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,205,504</span> (4.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,205,504\u001b[0m (4.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  unfreeze 20 MobileNetV2’s top layers\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5d825fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 228ms/step - accuracy: 0.5356 - loss: 0.7026 - val_accuracy: 0.5211 - val_loss: 0.6939\n",
      "Epoch 2/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 238ms/step - accuracy: 0.6094 - loss: 0.6594 - val_accuracy: 0.5351 - val_loss: 0.6957\n",
      "Epoch 3/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 215ms/step - accuracy: 0.6248 - loss: 0.6394 - val_accuracy: 0.5322 - val_loss: 0.6965\n",
      "Epoch 4/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 220ms/step - accuracy: 0.6351 - loss: 0.6335 - val_accuracy: 0.5409 - val_loss: 0.6968\n",
      "Epoch 5/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 217ms/step - accuracy: 0.6970 - loss: 0.5965 - val_accuracy: 0.5404 - val_loss: 0.6975\n",
      "Epoch 6/20\n",
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 219ms/step - accuracy: 0.6610 - loss: 0.6010 - val_accuracy: 0.5181 - val_loss: 0.7029\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "history_fine = model.fit(\n",
    "    train_aug.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cc2540d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m95/95\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 90ms/step\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step\n",
      "\u001b[1m  1/416\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 167ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 20:55:04.038136: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 6386880000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m416/416\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 197ms/step\n",
      "\n",
      "=== Classification Report — Train ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       spoof       0.51      1.00      0.67      1507\n",
      "     genuine       0.97      0.02      0.05      1507\n",
      "\n",
      "    accuracy                           0.51      3014\n",
      "   macro avg       0.74      0.51      0.36      3014\n",
      "weighted avg       0.74      0.51      0.36      3014\n",
      "\n",
      "\n",
      "=== Classification Report — Validation ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       spoof       0.56      0.62      0.59       950\n",
      "     genuine       0.46      0.39      0.42       760\n",
      "\n",
      "    accuracy                           0.52      1710\n",
      "   macro avg       0.51      0.51      0.51      1710\n",
      "weighted avg       0.51      0.52      0.52      1710\n",
      "\n",
      "\n",
      "=== Classification Report — Test ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       spoof       0.90      0.97      0.94     12008\n",
      "     genuine       0.13      0.04      0.06      1298\n",
      "\n",
      "    accuracy                           0.88     13306\n",
      "   macro avg       0.51      0.50      0.50     13306\n",
      "weighted avg       0.83      0.88      0.85     13306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred_train_raw = model.predict(X_train)\n",
    "y_pred_val_raw   = model.predict(X_val)\n",
    "y_pred_test_raw  = model.predict(X_test)\n",
    "\n",
    "# 2) Convert to discrete labels\n",
    "def to_labels(probs):\n",
    "    # binary-probabilities => threshold at 0.5\n",
    "    return (probs > 0.5).astype(int).ravel()\n",
    "\n",
    "y_pred_train = to_labels(y_pred_train_raw)\n",
    "y_pred_val   = to_labels(y_pred_val_raw)\n",
    "y_pred_test  = to_labels(y_pred_test_raw)\n",
    "\n",
    "# 3) Define class names\n",
    "target_names = ['spoof', 'genuine']\n",
    "\n",
    "# 4) Loop through each split\n",
    "for split_name, y_true, y_pred in [\n",
    "    ('Train',      y_train, y_pred_train),\n",
    "    ('Validation', y_val,   y_pred_val),\n",
    "    ('Test',       y_test,  y_pred_test)\n",
    "]:\n",
    "    print(f\"\\n=== Classification Report — {split_name} ===\")\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
